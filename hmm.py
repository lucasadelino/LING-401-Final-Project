import nltk
import nltk.corpus
import pandas as pd
import numpy as np

def hmm(corpus:list)->dict:
    """Trains a hidden markov model from input corpus. Returns a dictionary 
    containing transition and emission matrices.
    
    The transition matrix t represents the probability that a given tag is 
    followed by another. The emission matrix e represents the probability that
    a given tag generates a corresponding word.
    
    The input corpus must be in the form of a list of lists of tuples. 
    Each inner list corresponds to a sentence, and each tuple must consist of 
    exactly two elements: a word and its tag (which is basically the format
    generated by NLTK's tagged_sents() method)."""

    print('Training Hidden Markov Model from input corpus...')

    # Convert corpus to lowercase and add sentence markers
    # This process will also convert our corpus from a list of lists to just
    # one big list
    print("1/4 - Converting to lowercase and adding sentence start markers...")
    corpus_copy = []
    for sentence in corpus:
        # Add <s> marker
        corpus_copy.append(('<s>', '<s>'))
        for word, tag in sentence:
            # Convert word to lowercase
            corpus_copy.append((word.lower(), tag))
    
    # Use our copy in place of the original corpus
    corpus = corpus_copy

    # Count frequencies of words, tags, and bigrams made out of tags
    # For this, we will create several helper variables:
    print("2/4 - Counting frequencies...")
    word_tag_counts = dict(nltk.FreqDist(corpus).most_common())
    # A dictionary with words and how many times they appear
    word_counts = dict(nltk.FreqDist([word for word, _ in corpus]).most_common())
    # A list of tags only (without words)
    tags = [tag for _, tag in corpus]
    # A list of tuples consisting of a tag and how many times that tag appears
    tag_counts = nltk.FreqDist(tags).most_common()
    # A dict with tag bigrams and how many times they appear
    tag_bigram_counts = dict(nltk.FreqDist(nltk.bigrams(tags)).most_common())
    
    print('3/4 - Creating matrices...')
    # Create lists of tag and word labels, to serve as df row and column names 
    tag_labels = [tag for tag, _ in tag_counts]
    word_labels = [word for word, _ in word_counts.items()]
    
    t = [] # Transmission matrix
    # Populate transmission matrix from bigram counts
    for tag_i, count in tag_counts:
        row = []
        for tag_j, _ in tag_counts:
            row.append((tag_bigram_counts.get((tag_i, tag_j), 0))/count)
        t.append(row)
        
    e = [] # Emission matrix
    # Populate emission matrix from counts of tagged words
    for tag, t_count in tag_counts:
        row = []
        for word, _ in word_counts.items():
            row.append(word_tag_counts.get((word, tag), 0)/t_count)
        e.append(row)

    # Make matrices t and e into dataframes
    t_df = pd.DataFrame(t, columns=tag_labels)
    t_df.set_index([tag_labels], inplace=True)
    e_df = pd.DataFrame(e, columns=word_labels)
    e_df.set_index([tag_labels], inplace=True)
    # Having converted all those tagged sentences into one big list means that 
    # we counted bigrams where the last word of a given sentence preceded the
    # <s> marker at the beginning of the next sentence. These bigrams shouldn't
    # exist, though, since these sentences don't necessarily follow each other
    # (and we're not interested in chaining together sentences any way).
    # As such, we need to drop those from the dataframes:
    t_df.drop('<s>', axis=1)
    e_df.drop('<s>', axis=0)
    e_df.drop('<s>', axis=1)
    # FOR DEBUGGING: uncomment to save the matrices as .CSV files
    #t_df.to_csv('transmission.csv')
    #e_df.to_csv('emission.csv')
    
    print('4/4 - Done!')
    return {'transmission': t_df, 'emission': e_df}